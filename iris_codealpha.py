# -*- coding: utf-8 -*-
"""Iris_CodeAlpha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qjphCsZ4mDtxuWISO14yEiMjr9rS7Umw
"""

# Numerical operations
import numpy as np

# Data handling
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Machine Learning utilities
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Evaluation metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

# Load Iris dataset
df = pd.read_csv("Iris.csv")

# Drop ID column if present
if "Id" in df.columns:
    df.drop("Id", axis=1, inplace=True)

df.head()

# Dataset shape
print("Dataset Shape:", df.shape)

# Dataset information
df.info()

# Statistical summary
df.describe()

# Class distribution
df["Species"].value_counts()

# Input features
X = df.drop("Species", axis=1)

# Output label
y = df["Species"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,      # 80% training, 20% testing
    random_state=42,    # reproducibility
    stratify=y          # preserves class distribution
)

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d')

species_code = {
    "Iris-setosa": 0,
    "Iris-versicolor": 1,
    "Iris-virginica": 2
}

colors = y.map(species_code)

ax.scatter(
    df["SepalLengthCm"],
    df["PetalLengthCm"],
    df["PetalWidthCm"],
    c=colors
)

ax.set_xlabel("Sepal Length (cm)")
ax.set_ylabel("Petal Length (cm)")
ax.set_zlabel("Petal Width (cm)")
ax.set_title("3D Visualization of Iris Dataset")

plt.show()

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

lr_pred = lr_model.predict(X_test_scaled)
lr_accuracy = accuracy_score(y_test, lr_pred)

print("Logistic Regression Accuracy:", lr_accuracy)

svm_model = SVC(kernel="rbf")
svm_model.fit(X_train_scaled, y_train)

svm_pred = svm_model.predict(X_test_scaled)
svm_accuracy = accuracy_score(y_test, svm_pred)

print("SVM Accuracy:", svm_accuracy)

rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42
)

rf_model.fit(X_train, y_train)

rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

print("Random Forest Accuracy:", rf_accuracy)

param_grid = {
    "C": [0.1, 1, 10, 100],
    "gamma": [0.01, 0.1, 1, "scale"],
    "kernel": ["rbf"]
}

svm_grid = GridSearchCV(
    SVC(),
    param_grid,
    cv=5,
    scoring="accuracy"
)

svm_grid.fit(X_train_scaled, y_train)

print("Best Parameters:", svm_grid.best_params_)
print("Best CV Accuracy:", svm_grid.best_score_)

best_svm = svm_grid.best_estimator_

svm_tuned_pred = best_svm.predict(X_test_scaled)

print("Tuned SVM Test Accuracy:", accuracy_score(y_test, svm_tuned_pred))
print(classification_report(y_test, svm_tuned_pred))

cm = confusion_matrix(y_test, svm_tuned_pred)

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=best_svm.classes_
)

disp.plot()
plt.title("Confusion Matrix – Tuned SVM")
plt.show()

lr_cv = cross_val_score(lr_model, X_train_scaled, y_train, cv=5)
svm_cv = cross_val_score(best_svm, X_train_scaled, y_train, cv=5)
rf_cv = cross_val_score(rf_model, X_train, y_train, cv=5)

print("Logistic Regression CV Mean Accuracy:", lr_cv.mean())
print("SVM CV Mean Accuracy:", svm_cv.mean())
print("Random Forest CV Mean Accuracy:", rf_cv.mean())

results_df = pd.DataFrame({
    "Model": ["Logistic Regression", "SVM", "Random Forest"],
    "Test Accuracy": [lr_accuracy, accuracy_score(y_test, svm_tuned_pred), rf_accuracy]
})

results_df

plt.figure(figsize=(7,4))
plt.barh(X.columns, rf_model.feature_importances_)
plt.xlabel("Importance Score")
plt.title("Feature Importance – Random Forest")
plt.show()

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

svm_2d = SVC(kernel="rbf", C=best_svm.C, gamma=best_svm.gamma)
svm_2d.fit(X_pca, y_train)

plt.figure(figsize=(7,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train.map(species_code))
plt.title("Decision Boundary Visualization (PCA + SVM)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

feature_importance = rf_model.feature_importances_

plt.figure(figsize=(7,4))
plt.barh(X.columns, feature_importance)
plt.xlabel("Importance Score")
plt.title("Feature Importance in Random Forest")
plt.show()

print("""
FINAL CONCLUSION:
1. Iris dataset is well-balanced and suitable for classification.
2. Feature scaling significantly improves linear and kernel models.
3. Random Forest achieved the highest accuracy.
4. Petal measurements are the most influential features.
5. Proper preprocessing and evaluation ensure reliable results.
""")

